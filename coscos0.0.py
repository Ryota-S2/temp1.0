import csv
import os
import random
from json import loads
from ragas.metrics import faithfulness, answer_relevancy
from ragas import evaluate
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI
from datasets import Dataset
import pdfplumber
import numpy as np

# ===== PDF â†’ CSVå¤‰æ› =====
def pdf_to_csv(pdf_file, csv_file="Book1.csv"):
    explanations = []
    with pdfplumber.open(pdf_file) as pdf:
        for i, page in enumerate(pdf.pages, start=1):
            text = page.extract_text()
            if text:
                for line in text.split("\n\n"):
                    line = line.strip()
                    if line:
                        explanations.append([line])
    with open(csv_file, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerows(explanations)

def load_explanations_from_csv(filename="Book1.csv"):
    explanations = []
    with open(filename, "r", encoding="utf-8") as f:
        reader = csv.reader(f)
        for row in reader:
            if row:
                explanations.append(row[0])
    return explanations


# ===== OpenAI API ã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ =====
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)


# ===== è§£èª¬æ–‡ã®æ„å‘³è£œæ­£ =====
def refine_explanation(raw_text: str, client: OpenAI) -> str:
    """æŠ½å‡ºæ–‡ã‚’è‡ªç„¶ã§æ„å‘³ã®é€šã‚‹èª¬æ˜æ–‡ã«ãƒªãƒ©ã‚¤ãƒˆ"""
    response = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {
                "role": "system",
                "content": (
                    "ã‚ãªãŸã¯æ•™è‚²æ•™æã®ç·¨é›†è€…ã§ã™ã€‚"
                    "ä»¥ä¸‹ã®æ–‡ç« ã‚’è‡ªç„¶ã§æ„å‘³ã®é€šã‚‹æ—¥æœ¬èªã«ç›´ã—ã¦ãã ã•ã„ã€‚"
                    "æ–‡ã®ã¤ãªãŒã‚Šã‚’è£œã„ã€èª­ã‚“ã§ç†è§£ã§ãã‚‹å½¢ã«ã—ã¦ãã ã•ã„ã€‚"
                    "æ­´å²ã‚„åœ°åãªã©ã®å›ºæœ‰åè©ã¯æ­£ç¢ºã•ã‚’ä¿ã¡ã€æ”¹å¤‰ã—ã™ããªã„ã‚ˆã†ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚"
                )
            },
            {"role": "user", "content": raw_text},
        ],
        temperature=0.0
    )
    refined_text = response.choices[0].message.content.strip()
    return refined_text


# ===== Streamlit UI =====
st.title("å…µåº«å­¦æ¤œå®šè©¦é¨“å¯¾ç­–ãƒ„ãƒ¼ãƒ«ï¼ˆæ„å‘³è£œæ­£ï¼‹5å•åŒæ™‚å‡ºé¡Œï¼‹ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ä»˜ãï¼‰")

uploaded_file = st.file_uploader("ã‚¯ã‚¤ã‚ºã«ä½¿ã†PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸ã‚“ã§ãã ã•ã„", type=["pdf"])

if uploaded_file is not None:
    pdf_path = "uploaded.pdf"
    with open(pdf_path, "wb") as f:
        f.write(uploaded_file.read())

    pdf_to_csv(pdf_path, "Book1.csv")
    st.session_state.explanations = load_explanations_from_csv("Book1.csv")
    st.success(f"PDFã‚’å¤‰æ›ã—ã¦ {len(st.session_state.explanations)} ä»¶ã®æ–‡ç« ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼")


# ===== ã‚¯ã‚¤ã‚ºå‡ºé¡Œå‡¦ç† =====
if "explanations" in st.session_state and st.session_state.explanations:
    explanations = st.session_state.explanations

    if "generated_answers" not in st.session_state or st.session_state.get("next_question", False):
        # 1ã¤é¸æŠ
        QuestionNum = random.randint(0, len(explanations) - 1)
        SelectedQuestion = explanations[QuestionNum]

        # ===== Step 1: æ„å‘³ã®é€šã‚‹è§£èª¬æ–‡ã«ãƒªãƒ©ã‚¤ãƒˆ =====
        st.write("ğŸ§  è§£èª¬æ–‡ã‚’æ•´ãˆã¦ã„ã¾ã™...")
        CleanedExplanation = refine_explanation(SelectedQuestion, client)

        # ===== Step 2: 5å•åŒæ™‚ç”Ÿæˆ =====
        prompt = f"""
ã‚ãªãŸã¯æ•™è‚²ç”¨ã‚¯ã‚¤ã‚ºä½œæˆAIã§ã™ã€‚
æ¬¡ã®è§£èª¬æ–‡ã‚’ã‚‚ã¨ã«å››æŠå•é¡Œã‚’5å•ä½œæˆã—ã¦ãã ã•ã„ã€‚
å„å•é¡Œã«ã¯ã€Œå•é¡Œæ–‡ã€ã€Œé¸æŠè‚¢4ã¤ã€ã€Œæ­£è§£ç•ªå·ã€ã€Œè§£èª¬ã€ã‚’å«ã‚ã¦ãã ã•ã„ã€‚

--- è§£èª¬æ–‡ ---
{CleanedExplanation}

å‡ºåŠ›å½¢å¼ï¼ˆJSONé…åˆ—ï¼‰:
[
  {{
    "Question": "ï½",
    "Choice1": "ï½",
    "Choice2": "ï½",
    "Choice3": "ï½",
    "Choice4": "ï½",
    "CorrectAnswer": 1,
    "Explanation": "ï½"
  }},
  ...
]
"""

        response = client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯æ­£ç¢ºã§æ•™è‚²çš„ãªã‚¯ã‚¤ã‚ºä½œæˆAIã§ã™ã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=0.0,
        )

        try:
            output_json = response.choices[0].message.content
            generated_answers = loads(output_json)
        except Exception as e:
            st.error(f"JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
            st.stop()

        st.session_state.generated_answers = generated_answers
        st.session_state.explanation = CleanedExplanation
        st.session_state.next_question = False

        # ===== ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ =====
        correct_texts = [q[f"Choice{q['CorrectAnswer']}"] for q in generated_answers]

        def cosine_similarity(a, b):
            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

        embeddings = [
            client.embeddings.create(input=text, model="text-embedding-3-small").data[0].embedding
            for text in correct_texts
        ]

        n = len(embeddings)
        similarities = [
            cosine_similarity(embeddings[i], embeddings[j])
            for i in range(n) for j in range(i + 1, n)
        ]
        avg_cosine_similarity = np.mean(similarities)
        st.session_state.avg_cosine_similarity = avg_cosine_similarity

        # ===== RAGASè©•ä¾¡ =====
        sample = generated_answers[0]
        data = Dataset.from_dict({
            "question": [sample["Question"]],
            "answer": [sample[f"Choice{sample['CorrectAnswer']}"]],
            "contexts": [[CleanedExplanation]],
        })

        result1 = evaluate(data, metrics=[faithfulness])
        result2 = evaluate(data, metrics=[answer_relevancy])

        st.session_state.faithfulness = result1["faithfulness"][0]
        st.session_state.answer_relevance = result2["answer_relevancy"][0]

    # ===== å‡ºåŠ› =====
    st.subheader("æ•´ãˆãŸè§£èª¬æ–‡")
    st.info(st.session_state.explanation)

    st.subheader("ç”Ÿæˆã•ã‚ŒãŸ5å•ï¼ˆå•é¡Œï¼‹è§£èª¬ä»˜ãï¼‰")
    for idx, q in enumerate(st.session_state.generated_answers, start=1):
        st.markdown(f"### å•é¡Œ {idx}: {q['Question']}")
        st.markdown(
            f"1. {q['Choice1']}  \n"
            f"2. {q['Choice2']}  \n"
            f"3. {q['Choice3']}  \n"
            f"4. {q['Choice4']}  \n"
            f"âœ… æ­£è§£: {q[f'Choice{q['CorrectAnswer']}']}"
        )
        st.info(f"è§£èª¬ï¼š{q['Explanation']}")
        st.write("---")

    # ===== ã‚¹ã‚³ã‚¢è¡¨ç¤º =====
    st.subheader("ã‚¹ã‚³ã‚¢ã¾ã¨ã‚")
    st.write(f"Faithfulnessã‚¹ã‚³ã‚¢: {st.session_state.faithfulness}")
    st.write(f"Answer Relevancyã‚¹ã‚³ã‚¢: {st.session_state.answer_relevance}")
    st.write(f"å¹³å‡ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼ˆå°ã•ã„ã»ã©å¤šæ§˜æ€§ãŒé«˜ã„ï¼‰: {st.session_state.avg_cosine_similarity:.4f}")

    if st.button("æ¬¡ã®å•é¡Œã¸"):
        st.session_state.next_question = True
        st.session_state.pop("generated_answers", None)
        st.session_state.pop("avg_cosine_similarity", None)
        st.rerun()

else:
    st.info("ã¾ãšã¯PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
